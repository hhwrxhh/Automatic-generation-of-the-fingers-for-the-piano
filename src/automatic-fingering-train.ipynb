{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11652347,"datasetId":7312501,"databundleVersionId":12121503},{"sourceType":"datasetVersion","sourceId":11780515,"datasetId":6814830,"databundleVersionId":12266724},{"sourceType":"datasetVersion","sourceId":10735025,"datasetId":6655885,"databundleVersionId":11086033},{"sourceType":"datasetVersion","sourceId":10549015,"datasetId":6527006,"databundleVersionId":10881390,"isSourceIdPinned":true},{"sourceType":"datasetVersion","sourceId":11685190,"datasetId":7334099,"databundleVersionId":12158884},{"sourceType":"datasetVersion","sourceId":11933007,"datasetId":6532140,"databundleVersionId":12442125}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from music21 import pitch\nfrom pprint import pprint, pformat\n\nimport os\nimport random\nimport csv\nimport math\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn.functional as F\n\n\nfrom typing import List\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:35:53.389253Z","iopub.execute_input":"2025-06-16T06:35:53.389555Z","iopub.status.idle":"2025-06-16T06:35:53.394392Z","shell.execute_reply.started":"2025-06-16T06:35:53.389536Z","shell.execute_reply":"2025-06-16T06:35:53.393612Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nBLOCK_LENGTH = 11\nFUTURE_LENGTH = 5\nFINGER_SIZE = 5\nBATCH_SIZE = 32\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\nDATA_DIR = \"/kaggle/input/pig-new\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:52:42.818381Z","iopub.execute_input":"2025-06-16T06:52:42.818672Z","iopub.status.idle":"2025-06-16T06:52:42.823510Z","shell.execute_reply.started":"2025-06-16T06:52:42.818651Z","shell.execute_reply":"2025-06-16T06:52:42.822830Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"block_future = [(11, 5)]\nhands = [\"right\", \"left\"]\ninterval_to_midi = {\n    # \"Unison\": 0,\n    # \"Minor Second\": 1,\n    # \"Major Second\": 2,\n    # \"Minor Third\": 3,\n    \"Major Third\": 4,\n    # \"Perfect Fourth\": 5,\n    # \"Tritone\": 6,\n    # \"Perfect Fifth\": 7,\n    # \"Minor Sixth\": 8,\n    # \"Major Sixth\": 9,\n    # \"Minor Seventh\": 10,\n    # \"Major Seventh\": 11,\n    # \"Octave\": 12\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:36:02.761301Z","iopub.execute_input":"2025-06-16T06:36:02.761919Z","iopub.status.idle":"2025-06-16T06:36:02.765529Z","shell.execute_reply.started":"2025-06-16T06:36:02.761898Z","shell.execute_reply":"2025-06-16T06:36:02.764815Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Pitch Utilities\n","metadata":{}},{"cell_type":"code","source":"def extract_pitch_info(pitch):\n    \"\"\"\n    Returns white key index and black key flag from pitch string.\n\n    The white key index is centered around C4 (i.e., C4 → 0, C5 → 7, C3 → -7).\n\n    Args:\n        pitch (str): Note like \"C4\", \"D#5\", \"Bb3\".\n\n    Returns:\n        tuple: (white_key_val, black_key), where black_key is 1 for sharp/flat, 0 otherwise.\n\n    \"\"\"\n\n    \n    # Extract the base note and octave\n    base_note = pitch[0]  # First character (e.g., \"C\", \"D\")\n    octave = 4  # Default octave is 4 (octave start from middle C)\n    \n    note_val = {\"C\": 0, \"D\": 1, \"E\": 2, \"F\": 3, \"G\": 4, \"A\": 5, \"B\": 6}\n    \n    # Is the key a black key right next to the base note? (e.g., C#4, D#4)\n    black_key = 0 # Default is white key\n\n    # To tuple that split white/black keys, for allowing the black_key to reduce the span needed \n    # Note: Minus 4 to center around C4 (Middle c); Times 7 to span octaves (7 semitones)\n    if pitch[1].isdigit(): # No sharp/flat like \"C4\"\n        note_val[base_note] += (int(pitch[1]) - 4) * 7\n    elif pitch[1] == \"#\": # Sharp(1 semitone up) like \"C#4\n        black_key = 1\n        note_val[base_note] += (int(pitch[2]) - 4) * 7\n    elif pitch[1] in [\"b\", \"-\"]: # Flat(1 semitone down) like \"Cb4\"\n        black_key = 1\n\n    return (note_val[base_note], black_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:36:22.331904Z","iopub.execute_input":"2025-06-16T06:36:22.332175Z","iopub.status.idle":"2025-06-16T06:36:22.337804Z","shell.execute_reply.started":"2025-06-16T06:36:22.332156Z","shell.execute_reply":"2025-06-16T06:36:22.336994Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_white_black_diff(pitch1, pitch2):\n    \"\"\"\n    Calculate the white-key distance between two pitches.\n    \n    Args:\n        pitch1, pitch2: Note pitch in the format of \"C4\", \"D#5\", etc.\n\n    Returns:\n        int: Semitone distance between two pitches\n    \"\"\"\n    \n    a = extract_pitch_info(pitch1) \n    b =  extract_pitch_info(pitch2)\n    return abs(a[0] - b[0]), a[1] + b[1] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:36:26.330302Z","iopub.execute_input":"2025-06-16T06:36:26.331024Z","iopub.status.idle":"2025-06-16T06:36:26.335799Z","shell.execute_reply.started":"2025-06-16T06:36:26.330985Z","shell.execute_reply":"2025-06-16T06:36:26.335066Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def pass_bounds(notes):\n    \"\"\"\n    Checks if any note in the list is out of the allowed MIDI range.\n    \"\"\"\n    \n    pass_range = False\n    for n in notes:\n        if not (n == 0 or (21 <= n < 108)):\n            pass_range = True\n    return pass_range\n\n\ndef interval_symmetry(piece, interval):\n    \"\"\"\n    Generates symmetrical piece by applying interval shifts across multiple octaves.\n    \n    Args:\n        piece (PianoPiece): The original piano piece object.\n        interval (int): The interval (in semitones) to use for symmetry transposition.\n\n    Returns:\n        list of PianoPiece: List of transposed, symmetrical versions of the input piece.\n    \"\"\"\n    \n    pieces = []\n    octaves = [-9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    for octave in octaves:\n        new_notes = [(n + (octave * interval)) if n != 0 else 0 for n in piece.notes]\n        if not pass_bounds(new_notes):\n            pieces.append(PianoPiece(new_notes, \n                                     piece.fingers, \n                                     piece.intervals, \n                                     piece.accidentals, \n                                     piece.ids, \n                                     \n                                     piece.durations,\n                                     piece.onset,\n                                     piece.offset,\n\n                                     piece.is_3_chord,\n                                     piece.is_4_chord,\n                                     piece.is_5_chord,\n                                     piece.chord_tonic,\n                                     piece.chord_sixth,\n                                     piece.chord_second_inversion,\n                                     \n                                     piece.white_diff,\n                                     piece.black_diff,\n                                     piece.file_name))\n\n    return pieces\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:36:39.140126Z","iopub.execute_input":"2025-06-16T06:36:39.140561Z","iopub.status.idle":"2025-06-16T06:36:39.148262Z","shell.execute_reply.started":"2025-06-16T06:36:39.140534Z","shell.execute_reply":"2025-06-16T06:36:39.147257Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Chord Detection and Annotation","metadata":{}},{"cell_type":"code","source":"def detect_triad_type(notes):\n    \"\"\"\n    Determines the type of chord based on a list of three note names.\n\n    Args:\n        notes (list of int): List of midi values.\n\n    Returns:\n        str: One of \"tonic\", \"sixth\", \"second_inversion\", or \"unknown\".\n    \"\"\"\n    \n    try:\n        midi_notes = sorted([pitch.Pitch(n).midi for n in notes])\n        intervals = [midi_notes[i] - midi_notes[0] for i in range(1, 3)]\n        structure = (0, *intervals)\n\n        if structure in [(0, 4, 7), (0, 3, 7)]:\n            return \"chord_tonic\"\n        elif structure in [(0, 3, 8), (0, 4, 9)]:\n            return \"chord_sixth\"\n        elif structure in [(0, 5, 9), (0, 5, 8)]:\n            return \"chord_second_inversion\"\n        else:\n            return \"unknown\"\n    except:\n        return \"unknown\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:37:05.486633Z","iopub.execute_input":"2025-06-16T06:37:05.487285Z","iopub.status.idle":"2025-06-16T06:37:05.492205Z","shell.execute_reply.started":"2025-06-16T06:37:05.487264Z","shell.execute_reply":"2025-06-16T06:37:05.491455Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def mark_chords_with_type(df):\n    \"\"\"\n    Adds one-hot encoded columns indicating chord type for triads.\n    Also adds binary flags: is_3_chord, is_4_chord, is_5_chord.\n\n    Args:\n        df: DataFrame with a piano piece.\n\n    Returns:\n        Updated DataFrame with new one-hot encoded columns and chord size flags.\n    \"\"\"\n    \n    group_counts = df.groupby([\"Onset\", \"Offset\"]).size().reset_index(name=\"count\")\n    chord_groups = group_counts[group_counts[\"count\"] >= 3][[\"Onset\", \"Offset\", \"count\"]]\n\n    df = df.merge(chord_groups.assign(is_chord=1), on=[\"Onset\", \"Offset\"], how=\"left\")\n    df[\"is_chord\"] = df[\"is_chord\"].fillna(0).astype(int)\n    df[\"count\"] = df[\"count\"].fillna(0).astype(int)\n\n    df[\"chord_type\"] = \"none\"\n\n    for (onset, offset), group_df in df.groupby([\"Onset\", \"Offset\"]):\n        if len(group_df) == 3:\n            pitches = group_df[\"PitchName\"].tolist()  \n            chord_type = detect_triad_type(pitches)\n            df.loc[(df[\"Onset\"] == onset) & (df[\"Offset\"] == offset), \"chord_type\"] = chord_type\n\n    df[\"chord_type\"] = df[\"chord_type\"].replace(\"none\", \"unknown\")\n\n    dummies = pd.get_dummies(df[\"chord_type\"], dtype='int')\n    for i in dummies.columns:\n        df[i] = dummies[i]\n\n    df[\"is_3_chord\"] = (df[\"count\"] == 3).astype(int)\n    df[\"is_4_chord\"] = (df[\"count\"] == 4).astype(int)\n    df[\"is_5_chord\"] = (df[\"count\"] == 5).astype(int)\n    \n    # dropping unnecessary columns\n    for i in [\"count\", \"chord_type\", \"unknown\", \"is_chord\"]:\n        if i in df.columns:\n            df = df.drop(i, axis=1)\n\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:37:09.476658Z","iopub.execute_input":"2025-06-16T06:37:09.477317Z","iopub.status.idle":"2025-06-16T06:37:09.484280Z","shell.execute_reply.started":"2025-06-16T06:37:09.477297Z","shell.execute_reply":"2025-06-16T06:37:09.483463Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def reorganize_fingers(df, hand=\"right\"):\n    \"\"\"\n    Reorganizes pitch data for chords or intervals.\n\n    For each chord, the notes (and its rows) are sorted in ascending order.\n    Example: Given a chord G4, E4, and C4 is reordered as C4, E4, G4.\n    \"\"\"\n\n    df = df.copy()\n    df[\"PitchPs\"] = [pitch.Pitch(p).ps for p in df[\"PitchName\"]]\n\n    sorted_df = df.sort_values(by=[\"Onset\", \"PitchPs\"])\n    sorted_df[\"ID\"] = range(len(sorted_df))\n    sorted_df = sorted_df.drop(columns=[\"PitchPs\"])\n    \n    return sorted_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:37:13.477366Z","iopub.execute_input":"2025-06-16T06:37:13.477927Z","iopub.status.idle":"2025-06-16T06:37:13.482339Z","shell.execute_reply.started":"2025-06-16T06:37:13.477904Z","shell.execute_reply":"2025-06-16T06:37:13.481542Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Modeling and Data Structures","metadata":{}},{"cell_type":"code","source":"class PianoPiece:\n    \"\"\"\n    A data structure representing a piano piece with associated musical and fingering information.\n\n    Attributes:\n        notes : MIDI values representing the notes in the piece.\n        fingers : Finger numbers corresponding to each note.\n        intervals : Interval distances between consecutive notes.\n        accidentals : Accidentals (sharps, flats, naturals) for each note.\n        ids : Unique identifiers for each note or event.\n        durations : Durations for each note in seconds.\n        onset : Onset times (in seconds) for each note.\n        offset : Offset times (in seconds) for each note.\n\n        white_diff : Movement in terms of white keys between notes.\n        black_diff : Movement in terms of black keys between notes.\n\n        chords : Whether the note belongs to a chord.\n        chord_tonic : Whether the note is in the root (tonic) chord.\n        chord_sixth : Whether the note is in a sixth chord.\n        chord_second_inversion : Whether the note is in a second inversion chord.\n\n        file_name : Name of the file from which the data was derived.\n    \"\"\"\n\n    def __init__(\n        self,\n        notes = None, fingers = None, intervals = None,\n        accidentals = None, ids = None,\n        durations = None, onset = None, offset = None,\n        is_3_chord = None, is_4_chord = None, is_5_chord = None,\n        chord_tonic = None, chord_sixth = None, chord_second_inversion = None,\n        white_diff = None, black_diff = None,\n        file_name: str = \"\"\n    ):\n        self.notes = notes\n        self.fingers = fingers\n        self.intervals = intervals\n        self.accidentals = accidentals\n        self.ids = ids\n        self.durations = durations\n        self.onset = onset\n        self.offset = offset\n\n        self.white_diff = white_diff\n        self.black_diff = black_diff\n\n        self.is_3_chord = is_3_chord\n        self.is_4_chord = is_4_chord\n        self.is_5_chord = is_5_chord\n        self.chord_tonic = chord_tonic\n        self.chord_sixth = chord_sixth\n        self.chord_second_inversion = chord_second_inversion\n\n        self.file_name = file_name\n\n    def get_features(self, input_features):\n        \"\"\"Returns selected features in aligned format for model input.\"\"\"\n    \n        feature_map = {\n            \"fingers\": self.fingers[:-1],\n            \"notes\": self.notes[:-1],\n            \"intervals\": self.intervals,\n            \n            \"accidentals_current\": self.accidentals[:-1],\n            \"accidentals_next\": self.accidentals[1:],\n            \"white_diff\": self.white_diff[:-1],\n            \"black_diff\": self.black_diff[:-1],\n            # \"chords\": self.chords[:-1],\n            \n            \"is_3_chord\": self.is_3_chord[:-1],\n            \"is_4_chord\": self.is_4_chord[:-1],\n            \"is_5_chord\": self.is_5_chord[:-1],\n            \"chord_tonic\": self.chord_tonic[:-1],\n            \"chord_sixth\": self.chord_sixth[:-1],\n            \"chord_second_inversion\": self.chord_second_inversion[:-1],\n        }\n        \n        # Detect invalid features\n        invalid_features = [name for name in input_features if name not in feature_map]\n        if invalid_features:\n            raise ValueError(f\"The following input features are invalid: {invalid_features}\")\n    \n        selected = [feature_map[name] for name in input_features]\n    \n        if not selected:\n            raise ValueError(\"No valid features provided for model input.\")\n    \n        return [list(x) for x in zip(*selected)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:37:32.108618Z","iopub.execute_input":"2025-06-16T06:37:32.109268Z","iopub.status.idle":"2025-06-16T06:37:32.121855Z","shell.execute_reply.started":"2025-06-16T06:37:32.109241Z","shell.execute_reply":"2025-06-16T06:37:32.121049Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def check_finger_ranges(df, hand, file_path):\n    \"\"\"\n    Validates that finger values in the DataFrame fall within the correct range \n    for the specified hand. \n\n    Args:\n        df: DataFrame with a piano piece\n        hand (str): Either 'right' or 'left', indicating the hand being checked.\n    \"\"\"\n    \n    df[\"Finger\"] = df[\"Finger\"].astype(str).str.split('_').str[0].astype(int)\n\n    if hand == \"right\":\n        invalid_rows = df[(df[\"Finger\"] < 1) | (df[\"Finger\"] > 5)]\n        if not invalid_rows.empty:\n            raise ValueError(f\"Invalid right-hand finger values found in {file_path}:\\n{invalid_rows}\")\n\n    elif hand == \"left\":\n        invalid_rows = df[(df[\"Finger\"] < -5) | (df[\"Finger\"] > -1)]\n        if not invalid_rows.empty:\n            raise ValueError(f\"Invalid left-hand finger values found in {file_path}:\\n{invalid_rows}\")\n\n    else:\n        raise ValueError(f\"Invalid hand type: {hand}. Expected 'right' or 'left'.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:37:38.182287Z","iopub.execute_input":"2025-06-16T06:37:38.182990Z","iopub.status.idle":"2025-06-16T06:37:38.187919Z","shell.execute_reply.started":"2025-06-16T06:37:38.182967Z","shell.execute_reply":"2025-06-16T06:37:38.187101Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def load_piano_piece(file_path, hand=\"right\", use_white_black=False, aug=False):\n    \"\"\"\n    Loads data from a file, performs feature engineering\n    and optionally applies data augmentation using interval transpositions.\n\n    Returns:\n        list of PianoPiece objects\n    \"\"\"\n    \n    df = pd.read_csv(file_path, header=None)\n    df.columns = [\"ID\", \"Onset\", \"Offset\", \"PitchName\", \"Column4\", \"Column5\", \"Beam\", \"Finger\"]\n    \n    if df.empty:\n        print(f\"No data for {filepath}\")\n        return []\n    df = df.drop(columns=[\"Column4\", \"Column5\"])\n    \n    df[\"Onset\"] = df[\"Onset\"].astype(float)\n    df[\"Offset\"] = df[\"Offset\"].astype(float)\n    \n    # Select hand\n    df = df[df[\"Beam\"] == (0 if hand == \"right\" else 1)]\n    df = df.reset_index(drop=True)\n    df[\"Finger\"] = df[\"Finger\"].astype(str).str.split('_').str[0].astype(int)\n    df = reorganize_fingers(df, hand=hand)\n    check_finger_ranges(df, hand, file_path)\n    \n    # numeric pitch and accidental flag\n    df[\"Note\"] = df[\"PitchName\"].apply(lambda x: pitch.Pitch(x).ps)\n    df[\"Accidental\"] = df[\"PitchName\"].apply(lambda x: int(pitch.Pitch(x).accidental is None))\n\n    # prepare diff columns\n    df[\"white_diff\"] = 0\n    df[\"black_diff\"] = 0\n    \n    if use_white_black:\n        for i in range(1, len(df)):\n            w, b = get_white_black_diff(df.loc[i-1,\"PitchName\"], df.loc[i,\"PitchName\"])\n            df.loc[i, \"white_diff\"] = w\n            df.loc[i, \"black_diff\"] = b\n\n    chord_labels = [\n        \"is_3_chord\", \"chord_tonic\", \"chord_sixth\", \"chord_second_inversion\",\n        \"is_4_chord\", \"is_5_chord\",\n    ]\n    for lbl in chord_labels:\n        df[lbl] = 0\n    df = mark_chords_with_type(df)\n\n    df[\"Duration\"] = (df[\"Offset\"] - df[\"Onset\"]).round(2)\n    \n    notes = df[\"Note\"].tolist()\n    fingers = df[\"Finger\"].tolist()\n    white_diff = df[\"white_diff\"].tolist()\n    black_diff = df[\"black_diff\"].tolist()\n    accidentals = df[\"Accidental\"].tolist()\n    ids = df[\"ID\"].astype(int).tolist()\n    onset = df[\"Onset\"].round(2).tolist()\n    offset = df[\"Offset\"].round(2).tolist()\n    \n    is_3_chord = df[\"is_3_chord\"].tolist()\n    is_4_chord = df[\"is_4_chord\"].tolist()\n    is_5_chord = df[\"is_5_chord\"].tolist()\n    chord_tonic = df[\"chord_tonic\"].tolist()\n    chord_sixth = df[\"chord_sixth\"].tolist()\n    chord_second_inversion = df[\"chord_second_inversion\"].tolist()\n    \n    intervals = np.diff(np.array(notes, dtype=int)).tolist()\n\n    # normalize fingers for loss calculation\n    if hand == \"right\":\n        fingers = [f - 1 for f in fingers]\n    else:\n        fingers = [-f - 1 for f in fingers]\n\n    piece = PianoPiece(\n        notes=notes,\n        fingers=fingers,\n        intervals=intervals,\n        accidentals=accidentals,\n        ids=ids,\n        # durations=durations,\n        onset=onset,\n        offset=offset,\n        white_diff=white_diff,\n        black_diff=black_diff,\n        \n        is_3_chord=is_3_chord,\n        is_4_chord=is_4_chord,\n        is_5_chord=is_5_chord,\n        chord_tonic=chord_tonic,\n        chord_sixth=chord_sixth,\n        chord_second_inversion=chord_second_inversion,\n        file_name=file_path\n    )\n\n    if aug:\n        pieces = []\n        for interval in interval_to_midi.values():\n            pieces.extend(interval_symmetry(piece, interval))\n        return pieces\n    else:\n        return [piece]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:37:39.632480Z","iopub.execute_input":"2025-06-16T06:37:39.633276Z","iopub.status.idle":"2025-06-16T06:37:39.644220Z","shell.execute_reply.started":"2025-06-16T06:37:39.633246Z","shell.execute_reply":"2025-06-16T06:37:39.643565Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def slide_window_future_gen(input_list, window_size, future_size):\n    \"\"\"\n    Generates sliding windows of fixed size from the input list, with the future part of the window set to zero.\n    \n    Args:\n        input_list (list): A list of input data (e.g., finger positions, intervals).\n        window_size (int): The length of the sliding window.\n        future_size (int): The number of future time steps to mask (set to zero).\n        \n    Yields:\n        list: A sliding window of size `window_size`, with the last `future_size` elements set to zero.\n    \"\"\"\n    \n    for start in range(len(input_list) - window_size + 1):\n        full_list = input_list[start : start + window_size]\n        \n        for i in range(window_size-future_size, window_size):\n            full_list[i][0] = 0\n        \n        yield full_list\n\n\ndef prepare_inputs(file_paths, hand=\"right\",  input_features=None, aug=False):\n    \"\"\"\n    Prepare inputs for the neural network.\n\n    Returns:\n        tuple: \n            - inputs (list): A list of input sequences for the neural network.\n            - labels (list): A list of target labels (finger positions) corresponding to the input sequences.\n            - processed_data (dict): A dictionary mapping each filename to the number of pieces processed.\n    \"\"\"\n\n    inputs = []\n    labels = []\n    processed_data = {}\n    vector_list = []\n\n    if input_features is None:\n        input_features = [\"fingers\", \"intervals\", \"accidentals_current\", \"accidentals_next\"]\n        \n    # for filename in tqdm(sorted(filenames), desc=\"Processing Files\"):\n    for filename in sorted(file_paths):\n        vector_list = []\n        \n        if \"white_diff\" in input_features or \"black_diff\" in input_features:\n            pieces = load_piano_piece(filename, hand, use_white_black=True, aug=aug)\n        else:\n            pieces = load_piano_piece(filename, hand, aug=aug)\n            \n        for piece in pieces:\n            feature_matrix = piece.get_features(input_features)\n            vector_list.append(feature_matrix)\n            \n        processed_data[filename] = len(pieces)\n        \n        for i in range(len(vector_list)):\n            inputs.extend(\n                [l for l in slide_window_future_gen(vector_list[i], BLOCK_LENGTH, FUTURE_LENGTH)]\n            )\n            labels.extend(\n                [f for f in pieces[i].fingers[BLOCK_LENGTH - FUTURE_LENGTH : -FUTURE_LENGTH]]\n            )\n\n        \n    return inputs, labels, processed_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:38:51.095906Z","iopub.execute_input":"2025-06-16T06:38:51.096179Z","iopub.status.idle":"2025-06-16T06:38:51.103731Z","shell.execute_reply.started":"2025-06-16T06:38:51.096160Z","shell.execute_reply":"2025-06-16T06:38:51.103017Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Loading and splitting data ","metadata":{}},{"cell_type":"code","source":"class PianoFingeringDataset(Dataset):\n    \"\"\"\n    A dataset class for loading piano fingering data, designed for use with PyTorch DataLoader.\n    \"\"\"\n    \n    def __init__(self, file_paths, hand=\"right\", input_features=None, aug=False):\n        self.input_list, self.label_list, self.processed_data = prepare_inputs(file_paths, hand, input_features, aug)\n\n    def __len__(self):\n        return len(self.input_list)\n\n    def __getitem__(self, idx):\n        \n        x = torch.tensor(self.input_list[idx], dtype=torch.float32)\n        y = torch.tensor(self.label_list[idx], dtype=torch.int64)  \n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:43:27.562554Z","iopub.execute_input":"2025-06-16T06:43:27.563220Z","iopub.status.idle":"2025-06-16T06:43:27.568042Z","shell.execute_reply.started":"2025-06-16T06:43:27.563197Z","shell.execute_reply":"2025-06-16T06:43:27.567355Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def split_files(files, train_ratio, val_ratio, test_ratio):\n    train_files, temp_files = train_test_split(files, \n                                               test_size=(val_ratio + test_ratio), \n                                               random_state=48)\n    val_files, test_files = train_test_split(temp_files, \n                                             test_size=(test_ratio / (val_ratio + test_ratio)), \n                                             random_state=48)\n\n    return train_files, val_files, test_files\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:39:21.597356Z","iopub.execute_input":"2025-06-16T06:39:21.597618Z","iopub.status.idle":"2025-06-16T06:39:21.601867Z","shell.execute_reply.started":"2025-06-16T06:39:21.597602Z","shell.execute_reply":"2025-06-16T06:39:21.601228Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def prepare_data(data_dir, batch_size, train_ratio, val_ratio, test_ratio, hand=\"right\", input_features=None, aug=False):\n    \"\"\"\n    Prepares the data by splitting it into train, validation, and test sets, then creates DataLoader objects.\n    \"\"\"\n\n    all_files = os.listdir(data_dir)\n\n    full_paths = [os.path.join(data_dir, f) for f in all_files]\n    \n    print(f\"Working with {hand} hand!\")\n    train_files, val_files, test_files = split_files(full_paths, train_ratio, val_ratio, test_ratio)\n\n    print(f\"Train set {len(train_files)}\")\n    print(f\"Vaidation set {len(val_files)}\")\n    print(f\"Test set {len(test_files)}\\n\")\n    \n    train_dataset = PianoFingeringDataset(train_files, hand, input_features, aug)\n    val_dataset = PianoFingeringDataset(val_files, hand, input_features, aug)\n    test_dataset = PianoFingeringDataset(test_files, hand, input_features)\n\n    if aug:\n        len_train = 0\n        for i in train_dataset.processed_data.values():\n            len_train += int(i)\n        print(f\"Train set after aug {len_train}\")\n        \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n    return train_loader, val_loader, test_loader, train_files, val_files, test_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:39:56.162258Z","iopub.execute_input":"2025-06-16T06:39:56.162917Z","iopub.status.idle":"2025-06-16T06:39:56.169021Z","shell.execute_reply.started":"2025-06-16T06:39:56.162894Z","shell.execute_reply":"2025-06-16T06:39:56.168340Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, block_length, future_length, num_layers=1):\n        super(LSTM, self).__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.block_length = block_length\n        self.future_length = future_length\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True, num_layers=num_layers)\n        self.lambda_layer_idx = block_length - future_length - 1\n        self.dropout = nn.Dropout(p=0.4)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  \n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)  \n        selected_output = lstm_out[:, self.lambda_layer_idx, :]  \n        logits = self.fc(selected_output)  \n        probabilities = self.softmax(logits)\n        return probabilities\n\n    def __str__(self):\n        return f\"LSTM(h={self.hidden_size}, layers={self.num_layers}, block={self.block_length}, future={self.future_length})\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:40:12.505944Z","iopub.execute_input":"2025-06-16T06:40:12.506208Z","iopub.status.idle":"2025-06-16T06:40:12.512465Z","shell.execute_reply.started":"2025-06-16T06:40:12.506186Z","shell.execute_reply":"2025-06-16T06:40:12.511740Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class GRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, block_length, future_length, num_layers=1):\n        super(GRU, self).__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.block_length = block_length\n        self.future_length = future_length\n        self.num_layers = num_layers\n        \n        self.gru = nn.GRU(input_size, hidden_size, bidirectional=True, batch_first=True, num_layers=num_layers)\n        self.lambda_layer_idx = block_length - future_length - 1\n        self.dropout = nn.Dropout(p=0.4)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  \n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)  \n        selected_output = gru_out[:, self.lambda_layer_idx, :]  \n        logits = self.fc(selected_output)  \n        probabilities = self.softmax(logits)\n        return probabilities\n\n    def __str__(self):\n        return f\"GRU(h={self.hidden_size}, layers={self.num_layers}, block={self.block_length}, future={self.future_length})\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:40:19.415039Z","iopub.execute_input":"2025-06-16T06:40:19.415305Z","iopub.status.idle":"2025-06-16T06:40:19.421473Z","shell.execute_reply.started":"2025-06-16T06:40:19.415287Z","shell.execute_reply":"2025-06-16T06:40:19.420744Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def train_model(model, device, train_loader, val_loader, num_epochs,\n                lr=0.001, weights=None, name=None, log_file=None):\n    \n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss(weight=weights.to(device) if weights is not None else None)\n\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n\n    best_val_accuracy = 0.0\n    best_model_state = None\n\n    # List to store logs for each epoch\n    log_data = []\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n\n        # Training phase\n        model.train()\n        epoch_loss = 0.0\n        correct = 0\n        total = 0\n\n        for idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n        epoch_loss /= total\n        train_accuracy = correct / total\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for idx, (inputs, labels) in enumerate(val_loader):\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_loss /= val_total\n        val_accuracy = val_correct / val_total\n\n        epoch_time = time.time() - start_time\n\n        # Print to console\n        # if (epoch + 1) % 5 == 0:\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n              f\"Train Loss: {epoch_loss:.4f}, \"\n              f\"Train Accuracy: {train_accuracy:.4f}, \"\n              f\"Time: {epoch_time:.2f} sec \")\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n              f\"Val Loss: {val_loss:.4f}, \"\n              f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n\n        # Store the data for logging\n        log_data.append({\n            'Epoch': epoch + 1,\n            'Train Loss': round(epoch_loss, 3),\n            'Train Accuracy': round(train_accuracy, 3),\n            'Val Loss': round(val_loss, 3),\n            'Val Accuracy': round(val_accuracy, 3)\n        })\n\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            best_model_state = model.state_dict().copy()\n\n    if name and best_model_state:\n        print(f\"Saving best model with validation accuracy {best_val_accuracy:.4f} as {name}\")\n        torch.save({\n            'model_state_dict': best_model_state,\n            'model_config': {\n                'input_size': model.input_size,\n                'hidden_size': model.hidden_size,\n                'output_size': model.fc.out_features,\n                'block_length': model.block_length,\n                'future_length': model.future_length,\n                'num_layers': model.num_layers\n            }\n        }, name)\n\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n\n    # Convert the log data to a DataFrame and save as CSV\n    # log_df = pd.DataFrame(log_data)\n    # log_df.to_csv(log_file, index=False)\n    # print(f\"Training logs saved to {log_file}\")\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:50:26.741790Z","iopub.execute_input":"2025-06-16T06:50:26.742043Z","iopub.status.idle":"2025-06-16T06:50:26.752898Z","shell.execute_reply.started":"2025-06-16T06:50:26.742026Z","shell.execute_reply":"2025-06-16T06:50:26.752236Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"features_by_hand = {\n    # lstm best features\n    # \"right\":  ['fingers', 'intervals',  'chord_tonic', 'chord_sixth', 'chord_second_inversion'],\n    # \"left\":  ['fingers', 'intervals',  'chord_tonic', 'chord_sixth', 'chord_second_inversion'],\n\n    # gru best features\n    \"right\":  ['fingers', 'intervals', \"accidentals_current\", \"accidentals_next\", 'chord_tonic', 'chord_sixth', 'chord_second_inversion'],\n    \"left\":  ['fingers','intervals', \"accidentals_current\", \"accidentals_next\",  'chord_tonic', 'chord_sixth', 'chord_second_inversion'],\n}\n\nDATA_DIR = \"/kaggle/input/pig-new\"\n# DATA_DIR = \"/kaggle/input/pig-own-data\"\n# DATA_DIR = \"/kaggle/input/own-piano-data\"\n\ndata_loaders = {}\nfor hand in [\"right\", \"left\"]:\n    features = features_by_hand[hand]\n\n    train_loader, val_loader, test_loader, \\\n    train_files, val_files, test_files = prepare_data(\n        DATA_DIR, BATCH_SIZE, TRAIN_RATIO, VAL_RATIO, TEST_RATIO,\n        hand=hand, aug=False, input_features=features\n    )\n\n    data_loaders[hand] = {\n        \"train\": train_loader,\n        \"val\": val_loader,\n        \"test\": test_loader,\n        \"train_files\": train_files,\n        \"val_files\": val_files,\n        \"test_files\": test_files\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:43:30.876163Z","iopub.execute_input":"2025-06-16T06:43:30.876856Z","iopub.status.idle":"2025-06-16T06:43:46.922611Z","shell.execute_reply.started":"2025-06-16T06:43:30.876833Z","shell.execute_reply":"2025-06-16T06:43:46.922048Z"}},"outputs":[{"name":"stdout","text":"Working with right hand!\nTrain set 216\nVaidation set 46\nTest set 47\n\nWorking with left hand!\nTrain set 216\nVaidation set 46\nTest set 47\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# the best parameters were obtained as a result of experiments\nhyperparams_by_model = {\n    \"GRU\": {\n        \"hidden_size\": 16,\n        \"num_layers\": 3,\n    },\n    \"LSTM\": {\n        \"hidden_size\": 64,\n        \"num_layers\": 2,\n    },\n}\n\nmodels = {\n    \"right\": None,\n    \"left\": None,\n}\n\n\nfor hand in hands:\n    hparams = hyperparams_by_model[\"GRU\"]\n    print(f\"Training {hand} hand \")\n    \n    model = GRU(input_size=len(features_by_hand[hand]),\n                hidden_size=hparams['hidden_size'],\n                output_size=FINGER_SIZE,\n                block_length=BLOCK_LENGTH,\n                future_length=FUTURE_LENGTH,\n                num_layers=hparams['num_layers'])\n    \n    print(f\"INFO: {model}\")\n    \n    loaders = data_loaders[hand]\n    train_model(model, device, loaders[\"train\"], loaders[\"val\"], 20)\n    models[hand] = model\n    \n    print(\"__________________________________________________________\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T07:11:32.194129Z","iopub.execute_input":"2025-06-16T07:11:32.194537Z","iopub.status.idle":"2025-06-16T07:14:41.184361Z","shell.execute_reply.started":"2025-06-16T07:11:32.194514Z","shell.execute_reply":"2025-06-16T07:14:41.183739Z"}},"outputs":[{"name":"stdout","text":"Training right hand \nINFO: GRU(h=16, layers=3, block=11, future=5)\nEpoch [1/20], Train Loss: 1.3126, Train Accuracy: 0.5910, Time: 5.18 sec \nEpoch [1/20], Val Loss: 1.2539, Validation Accuracy: 0.6495\n\nEpoch [2/20], Train Loss: 1.2287, Train Accuracy: 0.6756, Time: 5.14 sec \nEpoch [2/20], Val Loss: 1.2416, Validation Accuracy: 0.6613\n\nEpoch [3/20], Train Loss: 1.2149, Train Accuracy: 0.6875, Time: 5.11 sec \nEpoch [3/20], Val Loss: 1.2359, Validation Accuracy: 0.6638\n\nEpoch [4/20], Train Loss: 1.2062, Train Accuracy: 0.6965, Time: 5.15 sec \nEpoch [4/20], Val Loss: 1.2448, Validation Accuracy: 0.6543\n\nEpoch [5/20], Train Loss: 1.1998, Train Accuracy: 0.7015, Time: 5.11 sec \nEpoch [5/20], Val Loss: 1.2293, Validation Accuracy: 0.6709\n\nEpoch [6/20], Train Loss: 1.1962, Train Accuracy: 0.7054, Time: 5.49 sec \nEpoch [6/20], Val Loss: 1.2344, Validation Accuracy: 0.6661\n\nEpoch [7/20], Train Loss: 1.1897, Train Accuracy: 0.7117, Time: 5.13 sec \nEpoch [7/20], Val Loss: 1.2287, Validation Accuracy: 0.6694\n\nEpoch [8/20], Train Loss: 1.1854, Train Accuracy: 0.7173, Time: 5.13 sec \nEpoch [8/20], Val Loss: 1.2263, Validation Accuracy: 0.6739\n\nEpoch [9/20], Train Loss: 1.1821, Train Accuracy: 0.7204, Time: 5.04 sec \nEpoch [9/20], Val Loss: 1.2276, Validation Accuracy: 0.6709\n\nEpoch [10/20], Train Loss: 1.1787, Train Accuracy: 0.7247, Time: 5.14 sec \nEpoch [10/20], Val Loss: 1.2202, Validation Accuracy: 0.6835\n\nEpoch [11/20], Train Loss: 1.1758, Train Accuracy: 0.7272, Time: 5.11 sec \nEpoch [11/20], Val Loss: 1.2266, Validation Accuracy: 0.6740\n\nEpoch [12/20], Train Loss: 1.1722, Train Accuracy: 0.7310, Time: 5.23 sec \nEpoch [12/20], Val Loss: 1.2250, Validation Accuracy: 0.6739\n\nEpoch [13/20], Train Loss: 1.1697, Train Accuracy: 0.7331, Time: 5.15 sec \nEpoch [13/20], Val Loss: 1.2202, Validation Accuracy: 0.6818\n\nEpoch [14/20], Train Loss: 1.1659, Train Accuracy: 0.7380, Time: 5.14 sec \nEpoch [14/20], Val Loss: 1.2285, Validation Accuracy: 0.6688\n\nEpoch [15/20], Train Loss: 1.1634, Train Accuracy: 0.7404, Time: 5.06 sec \nEpoch [15/20], Val Loss: 1.2227, Validation Accuracy: 0.6752\n\nEpoch [16/20], Train Loss: 1.1607, Train Accuracy: 0.7436, Time: 5.17 sec \nEpoch [16/20], Val Loss: 1.2141, Validation Accuracy: 0.6864\n\nEpoch [17/20], Train Loss: 1.1568, Train Accuracy: 0.7471, Time: 5.07 sec \nEpoch [17/20], Val Loss: 1.2173, Validation Accuracy: 0.6833\n\nEpoch [18/20], Train Loss: 1.1548, Train Accuracy: 0.7495, Time: 5.12 sec \nEpoch [18/20], Val Loss: 1.2159, Validation Accuracy: 0.6835\n\nEpoch [19/20], Train Loss: 1.1517, Train Accuracy: 0.7530, Time: 5.50 sec \nEpoch [19/20], Val Loss: 1.2098, Validation Accuracy: 0.6895\n\nEpoch [20/20], Train Loss: 1.1496, Train Accuracy: 0.7554, Time: 5.13 sec \nEpoch [20/20], Val Loss: 1.2115, Validation Accuracy: 0.6904\n\n__________________________________________________________\n\nTraining left hand \nINFO: GRU(h=16, layers=3, block=11, future=5)\nEpoch [1/20], Train Loss: 1.2660, Train Accuracy: 0.6392, Time: 4.25 sec \nEpoch [1/20], Val Loss: 1.2070, Validation Accuracy: 0.6941\n\nEpoch [2/20], Train Loss: 1.1812, Train Accuracy: 0.7223, Time: 4.22 sec \nEpoch [2/20], Val Loss: 1.1813, Validation Accuracy: 0.7247\n\nEpoch [3/20], Train Loss: 1.1665, Train Accuracy: 0.7376, Time: 4.21 sec \nEpoch [3/20], Val Loss: 1.1758, Validation Accuracy: 0.7284\n\nEpoch [4/20], Train Loss: 1.1566, Train Accuracy: 0.7474, Time: 4.27 sec \nEpoch [4/20], Val Loss: 1.1753, Validation Accuracy: 0.7253\n\nEpoch [5/20], Train Loss: 1.1488, Train Accuracy: 0.7560, Time: 4.23 sec \nEpoch [5/20], Val Loss: 1.1693, Validation Accuracy: 0.7334\n\nEpoch [6/20], Train Loss: 1.1428, Train Accuracy: 0.7614, Time: 4.54 sec \nEpoch [6/20], Val Loss: 1.1715, Validation Accuracy: 0.7310\n\nEpoch [7/20], Train Loss: 1.1387, Train Accuracy: 0.7658, Time: 4.24 sec \nEpoch [7/20], Val Loss: 1.1662, Validation Accuracy: 0.7376\n\nEpoch [8/20], Train Loss: 1.1342, Train Accuracy: 0.7704, Time: 4.28 sec \nEpoch [8/20], Val Loss: 1.1613, Validation Accuracy: 0.7382\n\nEpoch [9/20], Train Loss: 1.1316, Train Accuracy: 0.7722, Time: 4.27 sec \nEpoch [9/20], Val Loss: 1.1613, Validation Accuracy: 0.7417\n\nEpoch [10/20], Train Loss: 1.1285, Train Accuracy: 0.7748, Time: 4.22 sec \nEpoch [10/20], Val Loss: 1.1556, Validation Accuracy: 0.7458\n\nEpoch [11/20], Train Loss: 1.1199, Train Accuracy: 0.7854, Time: 4.31 sec \nEpoch [11/20], Val Loss: 1.1528, Validation Accuracy: 0.7479\n\nEpoch [12/20], Train Loss: 1.1156, Train Accuracy: 0.7892, Time: 4.26 sec \nEpoch [12/20], Val Loss: 1.1526, Validation Accuracy: 0.7496\n\nEpoch [13/20], Train Loss: 1.1120, Train Accuracy: 0.7928, Time: 4.37 sec \nEpoch [13/20], Val Loss: 1.1646, Validation Accuracy: 0.7369\n\nEpoch [14/20], Train Loss: 1.1094, Train Accuracy: 0.7958, Time: 4.34 sec \nEpoch [14/20], Val Loss: 1.1542, Validation Accuracy: 0.7500\n\nEpoch [15/20], Train Loss: 1.1065, Train Accuracy: 0.7991, Time: 4.30 sec \nEpoch [15/20], Val Loss: 1.1533, Validation Accuracy: 0.7463\n\nEpoch [16/20], Train Loss: 1.1038, Train Accuracy: 0.8019, Time: 4.29 sec \nEpoch [16/20], Val Loss: 1.1498, Validation Accuracy: 0.7527\n\nEpoch [17/20], Train Loss: 1.1024, Train Accuracy: 0.8034, Time: 4.29 sec \nEpoch [17/20], Val Loss: 1.1513, Validation Accuracy: 0.7492\n\nEpoch [18/20], Train Loss: 1.1012, Train Accuracy: 0.8046, Time: 4.30 sec \nEpoch [18/20], Val Loss: 1.1559, Validation Accuracy: 0.7488\n\nEpoch [19/20], Train Loss: 1.0994, Train Accuracy: 0.8057, Time: 4.23 sec \nEpoch [19/20], Val Loss: 1.1504, Validation Accuracy: 0.7519\n\nEpoch [20/20], Train Loss: 1.0975, Train Accuracy: 0.8086, Time: 4.25 sec \nEpoch [20/20], Val Loss: 1.1472, Validation Accuracy: 0.7554\n\n__________________________________________________________\n\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"## Evaluating","metadata":{}},{"cell_type":"code","source":"def update_state_with_prediction(old_state, finger_pred, new_vec, future_size):\n    pred = old_state[-future_size]\n    pred[0] = finger_pred  # Update the predicted finger\n\n    # Updating the state with the new vector as a tensor\n    new_state = torch.tensor([0] + new_vec, dtype=torch.float32)\n    old_state[-future_size] = pred\n    return old_state[1:] + [new_state]  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T06:59:20.009646Z","iopub.execute_input":"2025-06-16T06:59:20.010265Z","iopub.status.idle":"2025-06-16T06:59:20.014164Z","shell.execute_reply.started":"2025-06-16T06:59:20.010248Z","shell.execute_reply":"2025-06-16T06:59:20.013385Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def prepare_test_inputs(file_path, hand=\"right\", input_features=None):\n    inputs = []\n    labels = []\n    if \"white_diff\" in input_features or \"black_diff\" in input_features:\n        pieces = load_piano_piece(file_path, hand, use_white_black=True)\n    else:\n        pieces = load_piano_piece(file_path, hand)\n        \n    feature_matrix = pieces[0].get_features(input_features)\n        \n    inputs.append(feature_matrix)\n    labels.append(pieces[0].fingers)\n    \n    return inputs, labels, pieces[0].ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T07:09:56.252359Z","iopub.execute_input":"2025-06-16T07:09:56.252660Z","iopub.status.idle":"2025-06-16T07:09:56.257696Z","shell.execute_reply.started":"2025-06-16T07:09:56.252642Z","shell.execute_reply":"2025-06-16T07:09:56.257057Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"def predict_fingerings(input_list, label_list, model):\n    model.eval()\n    results = []\n\n    with torch.no_grad():\n        for test_vector, test_finger in zip(input_list, label_list):\n            init_state_b = [\n                torch.tensor([test_finger[i]] + test_vector[i], dtype=torch.float32)\n                for i in range(model.block_length - model.future_length)\n            ]\n            init_state_a = [\n                torch.tensor([0] + test_vector[i], dtype=torch.float32)\n                for i in range(model.block_length - model.future_length, model.block_length)\n            ]\n\n            init_state = init_state_b + init_state_a\n            num_intervals = len(test_vector)\n            temp_finger_res = []\n\n            for test_step in range(0, num_intervals - model.block_length + 1):\n                np_init_state = (\n                    torch.stack(init_state)\n                    .view(-1, model.block_length, model.input_size)\n                    .to(device)\n                )\n                \n                pred_prob = model(np_init_state)\n                finger_pred = torch.argmax(pred_prob, dim=1).item()\n                temp_finger_res.append(finger_pred)\n\n                if test_step < num_intervals - model.block_length - 1:\n                    next_vector = test_vector[test_step + model.block_length]\n                    init_state = update_state_with_prediction(\n                        init_state, finger_pred, next_vector, model.future_length\n                    )\n\n            temp_finger_res = (test_finger[: model.block_length - model.future_length] + temp_finger_res + test_finger[-model.future_length:])\n            results.append(temp_finger_res)\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T07:07:29.716452Z","iopub.execute_input":"2025-06-16T07:07:29.717102Z","iopub.status.idle":"2025-06-16T07:07:29.723584Z","shell.execute_reply.started":"2025-06-16T07:07:29.717082Z","shell.execute_reply":"2025-06-16T07:07:29.722979Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def evaluate_fingering(test_files, hand, model, input_features=None):\n    total_correct = 0\n    total_predictions = 0\n\n    for file in test_files:\n        \n        test_input_list, test_label_list, test_id_list = prepare_test_inputs(file, hand, input_features)\n        predicted_fingerings = predict_fingerings(test_input_list, test_label_list, model)\n        \n        flat_pred = [pred for pred in predicted_fingerings[0]]\n        # transformation = {4: 0, 0: 4, 3: 1, 1: 3}\n\n        # flat_pred = [transformation.get(pred, pred) for pred in flat_pred]\n\n        # print(flat_pred)\n        flat_label = [gt for gt in test_label_list[0]]\n        correct = sum(p == gt for p, gt in zip(flat_pred, flat_label))\n        total_correct += correct\n        total_predictions += len(flat_label)\n        \n        file_accuracy = correct / len(flat_label) if len(flat_label) > 0 else 0\n        print(f\"File: {file} | Accuracy: {file_accuracy:.4f}\")\n\n    overall_accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n    print(f\"Overall Accuracy: {overall_accuracy:.4f}\\n\")\n    return overall_accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T07:07:32.194928Z","iopub.execute_input":"2025-06-16T07:07:32.195421Z","iopub.status.idle":"2025-06-16T07:07:32.200767Z","shell.execute_reply.started":"2025-06-16T07:07:32.195398Z","shell.execute_reply":"2025-06-16T07:07:32.200143Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"for hand in [\"right\", \"left\"]:\n    print(f\"Evaluating {hand} hand\")\n    print(model)\n    \n    # excluding fingers feature because of evaluating\n    feature = features_by_hand[hand][1:]\n    \n    evaluate_fingering(test_files[:], hand, models[hand], feature)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T07:14:56.991726Z","iopub.execute_input":"2025-06-16T07:14:56.992003Z","iopub.status.idle":"2025-06-16T07:15:06.889598Z","shell.execute_reply.started":"2025-06-16T07:14:56.991984Z","shell.execute_reply":"2025-06-16T07:15:06.888961Z"}},"outputs":[{"name":"stdout","text":"Evaluating right hand\nGRU(h=16, layers=3, block=11, future=5)\nFile: /kaggle/input/pig-new/040-1_fingering.csv | Accuracy: 0.6667\nFile: /kaggle/input/pig-new/026-5_fingering.csv | Accuracy: 0.8576\nFile: /kaggle/input/pig-new/051-1_fingering.csv | Accuracy: 0.6359\nFile: /kaggle/input/pig-new/030-4_fingering.csv | Accuracy: 0.8889\nFile: /kaggle/input/pig-new/126-1_fingering.csv | Accuracy: 0.7442\nFile: /kaggle/input/pig-new/022-3_fingering.csv | Accuracy: 0.8477\nFile: /kaggle/input/pig-new/041-1_fingering.csv | Accuracy: 0.6389\nFile: /kaggle/input/pig-new/003-5_fingering.csv | Accuracy: 0.6041\nFile: /kaggle/input/pig-new/007-1_fingering.csv | Accuracy: 0.6703\nFile: /kaggle/input/pig-new/047-2_fingering.csv | Accuracy: 0.7135\nFile: /kaggle/input/pig-new/015-3_fingering.csv | Accuracy: 0.6536\nFile: /kaggle/input/pig-new/097-1_fingering.csv | Accuracy: 0.6741\nFile: /kaggle/input/pig-new/038-1_fingering.csv | Accuracy: 0.8158\nFile: /kaggle/input/pig-new/019-3_fingering.csv | Accuracy: 0.6870\nFile: /kaggle/input/pig-new/022-1_fingering.csv | Accuracy: 0.7822\nFile: /kaggle/input/pig-new/141-1_fingering.csv | Accuracy: 0.7423\nFile: /kaggle/input/pig-new/021-5_fingering.csv | Accuracy: 0.9059\nFile: /kaggle/input/pig-new/020-3_fingering.csv | Accuracy: 0.6556\nFile: /kaggle/input/pig-new/117-1_fingering.csv | Accuracy: 0.7362\nFile: /kaggle/input/pig-new/012-3_fingering.csv | Accuracy: 0.7500\nFile: /kaggle/input/pig-new/089-1_fingering.csv | Accuracy: 0.6230\nFile: /kaggle/input/pig-new/064-1_fingering.csv | Accuracy: 0.6198\nFile: /kaggle/input/pig-new/059-1_fingering.csv | Accuracy: 0.7826\nFile: /kaggle/input/pig-new/053-1_fingering.csv | Accuracy: 0.5920\nFile: /kaggle/input/pig-new/122-1_fingering.csv | Accuracy: 0.6952\nFile: /kaggle/input/pig-new/011-1_fingering.csv | Accuracy: 0.5878\nFile: /kaggle/input/pig-new/045-2_fingering.csv | Accuracy: 0.7598\nFile: /kaggle/input/pig-new/046-1_fingering.csv | Accuracy: 0.5934\nFile: /kaggle/input/pig-new/121-2_fingering.csv | Accuracy: 0.8375\nFile: /kaggle/input/pig-new/087-1_fingering.csv | Accuracy: 0.3788\nFile: /kaggle/input/pig-new/076-1_fingering.csv | Accuracy: 0.6812\nFile: /kaggle/input/pig-new/131-1_fingering.csv | Accuracy: 0.7389\nFile: /kaggle/input/pig-new/014-1_fingering.csv | Accuracy: 0.7006\nFile: /kaggle/input/pig-new/011-3_fingering.csv | Accuracy: 0.6689\nFile: /kaggle/input/pig-new/017-3_fingering.csv | Accuracy: 0.5372\nFile: /kaggle/input/pig-new/028-4_fingering.csv | Accuracy: 0.7676\nFile: /kaggle/input/pig-new/001-5_fingering.csv | Accuracy: 0.5320\nFile: /kaggle/input/pig-new/113-1_fingering.csv | Accuracy: 0.8632\nFile: /kaggle/input/pig-new/025-3_fingering.csv | Accuracy: 0.8186\nFile: /kaggle/input/pig-new/032-3_fingering.csv | Accuracy: 0.7507\nFile: /kaggle/input/pig-new/140-2_fingering.csv | Accuracy: 0.7941\nFile: /kaggle/input/pig-new/004-8_fingering.csv | Accuracy: 0.7561\nFile: /kaggle/input/pig-new/026-1_fingering.csv | Accuracy: 0.8449\nFile: /kaggle/input/pig-new/011-5_fingering.csv | Accuracy: 0.6959\nFile: /kaggle/input/pig-new/107-1_fingering.csv | Accuracy: 0.4306\nFile: /kaggle/input/pig-new/001-8_fingering.csv | Accuracy: 0.6480\nFile: /kaggle/input/pig-new/023-1_fingering.csv | Accuracy: 0.4932\nOverall Accuracy: 0.7220\n\nEvaluating left hand\nGRU(h=16, layers=3, block=11, future=5)\nFile: /kaggle/input/pig-new/040-1_fingering.csv | Accuracy: 0.8364\nFile: /kaggle/input/pig-new/026-5_fingering.csv | Accuracy: 0.8909\nFile: /kaggle/input/pig-new/051-1_fingering.csv | Accuracy: 0.9130\nFile: /kaggle/input/pig-new/030-4_fingering.csv | Accuracy: 0.8467\nFile: /kaggle/input/pig-new/126-1_fingering.csv | Accuracy: 0.7037\nFile: /kaggle/input/pig-new/022-3_fingering.csv | Accuracy: 0.8431\nFile: /kaggle/input/pig-new/041-1_fingering.csv | Accuracy: 0.5065\nFile: /kaggle/input/pig-new/003-5_fingering.csv | Accuracy: 0.7965\nFile: /kaggle/input/pig-new/007-1_fingering.csv | Accuracy: 0.7500\nFile: /kaggle/input/pig-new/047-2_fingering.csv | Accuracy: 0.6620\nFile: /kaggle/input/pig-new/015-3_fingering.csv | Accuracy: 0.9052\nFile: /kaggle/input/pig-new/097-1_fingering.csv | Accuracy: 0.8494\nFile: /kaggle/input/pig-new/038-1_fingering.csv | Accuracy: 0.8158\nFile: /kaggle/input/pig-new/019-3_fingering.csv | Accuracy: 0.7073\nFile: /kaggle/input/pig-new/022-1_fingering.csv | Accuracy: 0.8816\nFile: /kaggle/input/pig-new/141-1_fingering.csv | Accuracy: 0.7912\nFile: /kaggle/input/pig-new/021-5_fingering.csv | Accuracy: 0.8451\nFile: /kaggle/input/pig-new/020-3_fingering.csv | Accuracy: 0.6373\nFile: /kaggle/input/pig-new/117-1_fingering.csv | Accuracy: 0.7692\nFile: /kaggle/input/pig-new/012-3_fingering.csv | Accuracy: 0.9231\nFile: /kaggle/input/pig-new/089-1_fingering.csv | Accuracy: 0.6942\nFile: /kaggle/input/pig-new/064-1_fingering.csv | Accuracy: 0.6757\nFile: /kaggle/input/pig-new/059-1_fingering.csv | Accuracy: 0.8783\nFile: /kaggle/input/pig-new/053-1_fingering.csv | Accuracy: 0.5083\nFile: /kaggle/input/pig-new/122-1_fingering.csv | Accuracy: 0.7000\nFile: /kaggle/input/pig-new/011-1_fingering.csv | Accuracy: 0.8468\nFile: /kaggle/input/pig-new/045-2_fingering.csv | Accuracy: 0.7087\nFile: /kaggle/input/pig-new/046-1_fingering.csv | Accuracy: 0.6667\nFile: /kaggle/input/pig-new/121-2_fingering.csv | Accuracy: 0.6561\nFile: /kaggle/input/pig-new/087-1_fingering.csv | Accuracy: 0.7905\nFile: /kaggle/input/pig-new/076-1_fingering.csv | Accuracy: 0.7791\nFile: /kaggle/input/pig-new/131-1_fingering.csv | Accuracy: 0.7692\nFile: /kaggle/input/pig-new/014-1_fingering.csv | Accuracy: 0.9098\nFile: /kaggle/input/pig-new/011-3_fingering.csv | Accuracy: 0.6935\nFile: /kaggle/input/pig-new/017-3_fingering.csv | Accuracy: 0.9028\nFile: /kaggle/input/pig-new/028-4_fingering.csv | Accuracy: 0.8974\nFile: /kaggle/input/pig-new/001-5_fingering.csv | Accuracy: 0.6393\nFile: /kaggle/input/pig-new/113-1_fingering.csv | Accuracy: 0.7426\nFile: /kaggle/input/pig-new/025-3_fingering.csv | Accuracy: 0.8202\nFile: /kaggle/input/pig-new/032-3_fingering.csv | Accuracy: 0.6860\nFile: /kaggle/input/pig-new/140-2_fingering.csv | Accuracy: 0.7451\nFile: /kaggle/input/pig-new/004-8_fingering.csv | Accuracy: 0.6780\nFile: /kaggle/input/pig-new/026-1_fingering.csv | Accuracy: 0.9127\nFile: /kaggle/input/pig-new/011-5_fingering.csv | Accuracy: 0.8548\nFile: /kaggle/input/pig-new/107-1_fingering.csv | Accuracy: 0.7523\nFile: /kaggle/input/pig-new/001-8_fingering.csv | Accuracy: 0.6758\nFile: /kaggle/input/pig-new/023-1_fingering.csv | Accuracy: 0.8521\nOverall Accuracy: 0.7819\n\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}